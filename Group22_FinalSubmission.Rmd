---
title: "Group22 Final Code Submission"
author: "Group 22"
date: "2024-05-08"
output: html_document
---

```{r setup}

#load libraries
library(tidyverse)
library(stringi)
library(caret)
library(stringr)
library(ggplot2)
library(tm)
library(text2vec)
library(SnowballC)
library(glmnet)
library(vip)
library(quanteda)
library(dplyr)
library(caret)
library(ranger)
library(xgboost)
library(ROCR)

```


```{r data_setup, echo = False, warning = False}

#load data files

setwd("C:/UMCP BA/Sem 2/Data Mining and Predictive Analytics/Midterm Proj")
train_x <- read_csv("airbnb_train_x_2024.csv")    
train_y <- read_csv("airbnb_train_y_2024.csv")

test <- read_csv("airbnb_test_x_2024.csv")


#join the training y to the training x file
#also turn the target variables into factors
train <- cbind(train_x, train_y) %>%
  mutate(perfect_rating_score = as.factor(perfect_rating_score),
         high_booking_rate = as.factor(high_booking_rate))

```


```{r data_cleaning, echo=FALSE, warning = False}
test <- test %>%
  mutate(host_acceptance_rate = ifelse(grepl("%$", test$host_acceptance_rate), as.numeric(gsub("%", "", test$host_acceptance_rate)), 
                                       as.numeric(test$host_acceptance_rate)),
         host_acceptance_rate = if_else(is.na(host_acceptance_rate), 0, host_acceptance_rate),
         host_acceptance_rate = host_acceptance_rate/100)



# Identify columns with more than 5000 NA values in 'train'
columns_na_train <- names(which(colSums(is.na(train)) > 45000))

# For each column with many NA values, convert it into binary column
for (col in columns_na_train) {
  train <- train %>%
    mutate(!!paste0(col, "_present") := ifelse(!is.na(.data[[col]]), 1, 0))  # Remove the originalÂ column
  test <- test %>%
    mutate(!!paste0(col, "_present") := ifelse(!is.na(.data[[col]]), 1, 0))
}


clean_data <- function(data) {
  
  # Remove unnecessary columns
  data <- data %>%
    select(-c( zipcode,
               experiences_offered, host_name, 
               host_about, notes))
  
  # Impute missing values
  data <- data %>%
    mutate(
      cancellation_policy = ifelse(cancellation_policy %in% 
                                     c("strict", "super_strict_30"), "strict", cancellation_policy),
      host_response_time = if_else(is.na(host_response_time), "Unknown", host_response_time),
      host_neighbourhood = if_else(is.na(host_neighbourhood), host_location, host_neighbourhood),
      jurisdiction_names = if_else(is.na(jurisdiction_names), smart_location, jurisdiction_names),
      cleaning_fee = if_else(is.na(cleaning_fee), 0, cleaning_fee),
      accommodates = if_else(is.na(accommodates), mean(accommodates, na.rm = TRUE), accommodates),
      bathrooms = if_else(is.na(bathrooms), mean(bathrooms, na.rm = TRUE), bathrooms),
      bedrooms = if_else(is.na(bedrooms), mean(bedrooms, na.rm = TRUE), bedrooms),
      beds = if_else(is.na(beds), mean(beds, na.rm = TRUE), beds),
      host_response_rate = ifelse(is.na(host_response_rate), 0, host_response_rate),
      host_total_listings_count = if_else(is.na(host_total_listings_count), 0, host_total_listings_count),
      property_type = if_else(is.na(property_type), "Other", property_type),
      license =  if_else(is.na(license), "No License", license),
      host_acceptance_rate = if_else(is.na(host_acceptance_rate), 0, as.numeric(gsub("%", "",host_acceptance_rate))),
      room_type = if_else(is.na(room_type), "Other", room_type),
      bed_type = if_else(is.na(bed_type), "Other", bed_type),
      market = if_else(is.na(market), city, market),
      city = if_else(is.na(city), market, city),
      summary = if_else(is.na(summary), 'NoSummary', summary),
      neighborhood_group = if_else(is.na(neighborhood_group), city, neighborhood_group),
      square_feet = if_else(is.na(square_feet), mean(square_feet, na.rm = TRUE), bedrooms),
      security_deposit = case_when(
        security_deposit <= 250 ~ "Small",
        (security_deposit > 250) & (security_deposit <= 500) ~ "Medium",
        (security_deposit > 500) & (security_deposit <= 750) ~ "Moderate",
        (security_deposit > 700) & (security_deposit <= 1000) ~ "Large",
        is.na(security_deposit) ~ "Unstated"
      ),
      host_since = ifelse(is.na(host_since),first_review, host_since),
      host_since = as.Date(host_since, origin = "1899-12-30"),
      first_review = as.Date(first_review, origin = "1899-12-30"),
      price = ifelse(is.na(price), mean(price, na.rm = TRUE), price))
  
  data_week_check <- data %>%
    select(price, weekly_price) %>%
    filter(!is.na(weekly_price)) %>%
    mutate(ratio_weekly = weekly_price/price) %>%
    summarize(mean_ratio = mean(ratio_weekly))
  
  data_month_check <- data %>%
    select(price, monthly_price) %>%
    filter(!is.na(monthly_price)) %>%
    mutate(ratio_monthly = monthly_price/price) %>%
    summarize(mean_ratio = mean(ratio_monthly))
  
  data <- data %>%
    mutate(monthly_price = ifelse(is.na(monthly_price), price * data_month_check$mean_ratio , monthly_price),
           monthly_price = round(monthly_price,0),
           weekly_price = ifelse(is.na(weekly_price), price * data_week_check$mean_ratio , weekly_price),
           weekly_price = round(weekly_price,0))
  
  data <- data %>%
    select(-c(host_location))  # Drop unnecessary columns
  
  return(data)
}


# Apply the cleaning function to our dataset
train_clean <- clean_data(train)
test_clean <- clean_data(test)

## Code to make list of unique verifications
split_ver <- strsplit(as.character(train_clean$host_verifications), ",")
unique_verifications <- na.omit(unique(unlist(split_ver)))
```


```{r one_hot, echo=FALSE, warning = False}
#One hot encoding

cat_cols <- c('bed_type', 'cancellation_policy', 'host_response_time', 'property_type', 'room_type')

numeric_cols <- sapply(train_clean, is.numeric)
numeric_cols_test <- sapply(test_clean, is.numeric)

# train clean
train_clean <- train_clean %>%
  mutate(across(bed_type, as.factor)) %>%
  mutate(across(bed_type, ~relevel(., ref = "Real Bed"))) %>%  
  mutate(across(bed_type, ~as.numeric(.) - 1))  # convert factor levels to numeric

train_clean <- train_clean %>%
  mutate(across(cancellation_policy, as.factor)) %>%
  mutate(across(cancellation_policy, ~relevel(., ref = "flexible"))) %>%  
  mutate(across(cancellation_policy, ~as.numeric(.) - 1))  

train_clean <- train_clean %>%
  mutate(across(host_response_time, as.factor)) %>%
  mutate(across(host_response_time, ~relevel(., ref = "within an hour"))) %>%  
  mutate(across(host_response_time, ~as.numeric(.) - 1))  

train_clean <- train_clean %>%
  mutate(across(property_type, as.factor)) %>%
  mutate(across(property_type, ~relevel(., ref = "House"))) %>%  
  mutate(across(property_type, ~as.numeric(.) - 1))  

train_clean <- train_clean %>%
  mutate(across(room_type, as.factor)) %>%
  mutate(across(room_type, ~relevel(., ref = "Private room"))) %>%  
  mutate(across(room_type, ~as.numeric(.) - 1))  

# test clean
test_clean <- test_clean %>%
  mutate(across(bed_type, as.factor)) %>%
  mutate(across(bed_type, ~relevel(., ref = "Real Bed"))) %>%  
  mutate(across(bed_type, ~as.numeric(.) - 1))

test_clean <- test_clean %>%
  mutate(across(cancellation_policy, as.factor)) %>%
  mutate(across(cancellation_policy, ~relevel(., ref = "flexible"))) %>%
  mutate(across(cancellation_policy, ~as.numeric(.) - 1))

test_clean <- test_clean %>%
  mutate(across(host_response_time, as.factor)) %>%
  mutate(across(host_response_time, ~relevel(., ref = "within an hour"))) %>%
  mutate(across(host_response_time, ~as.numeric(.) - 1))

test_clean <- test_clean %>%
  mutate(across(property_type, as.factor)) %>%
  mutate(across(property_type, ~relevel(., ref = "House"))) %>%
  mutate(across(property_type, ~as.numeric(.) - 1))

test_clean <- test_clean %>%
  mutate(across(room_type, as.factor)) %>%
  mutate(across(room_type, ~relevel(., ref = "Private room"))) %>%
  mutate(across(room_type, ~as.numeric(.) - 1))
```

```{r 1st model: logistic regression model, echo=FALSE, warning = False}

calculate_score <- function(entry, verification_methods) {
  methods <- strsplit(entry, ",")[[1]]
  score <- sum(verification_methods %in% methods)
  return(score)
}

# Define verification methods
verification_methods <- unique(unlist(strsplit(train_clean$host_verifications, ",")))
unique_amenities<- unique(unlist(strsplit(train_clean$amenities, ",")))

# Calculate scores for each entry
train_clean$host_verification_score <- sapply(train_clean$host_verifications, calculate_score, verification_methods)

# Calculate score for each entry
train_clean$amenity_score <- sapply(train_clean$amenities, calculate_score, unique_amenities)

test_clean$host_verification_score <- sapply(test_clean$host_verifications, calculate_score, verification_methods)

# Calculate score for each entry
test_clean$amenity_score <- sapply(test_clean$amenities, calculate_score, unique_amenities)

## Model
##train-test split (70%-30% split)
train_clean_log <- train_clean[1:82067, ]
valid_clean_log <- train_clean[82068:nrow(train_clean), ] 
va_inds_log <- sample(nrow(train_clean_log), .3*nrow(train_clean_log))
tr_log <- train_clean_log[-va_inds_log,]
va_log <- train_clean_log[va_inds_log,]
tr_y_log <- tr_log$perfect_rating_score
va_y_log <- va_log$perfect_rating_score

tr_full_y_log <- train_clean_log$perfect_rating_score
va_full_y_log <- valid_clean_log$perfect_rating_score


logmodel <- glm(perfect_rating_score~ host_response_rate + host_listings_count + price + 
                  room_type + bedrooms + beds + cancellation_policy + amenity_score +
                  cleaning_fee + host_verification_score + guests_included +
                  availability_30 + accommodates*bathrooms + extra_people +
                  maximum_nights + bed_type*price + availability_365 + minimum_nights
                , data = tr_log, family = "binomial")

##Training performance

preds <- predict(logmodel, newdata = tr_log, type = "response")
classifications <- ifelse(preds > .5, "YES", "NO")

valid_classifications_train_log <- as.factor(classifications)
tr_check_log = factor(tr_y_log)


CM_train_log<- confusionMatrix(data = valid_classifications_train_log, #predictions
                      reference = tr_check_log,
                      positive = "YES"#actuals
                      )
TPR_train_log <- as.numeric(CM_train_log$byClass["Sensitivity"])
FPR_train_log <- 1 - as.numeric(CM_train_log$byClass["Specificity"])

##Generalization performance

preds <- predict(logmodel, newdata = va_log, type = "response")
classifications <- ifelse(preds > .5, "YES", "NO")

valid_classifications_log <- as.factor(classifications)
va_check_log = factor(va_y_log)


CM_log<- confusionMatrix(data = valid_classifications_log, #predictions
                      reference = va_check_log,
                      positive = "YES"#actuals
                      )
TPR_log <- as.numeric(CM_log$byClass["Sensitivity"])
FPR_log <- 1 - as.numeric(CM_log$byClass["Specificity"])

##Holdout performance
preds <- predict(logmodel, newdata = valid_clean_log, type = "response")
classifications <- ifelse(preds > .5, "YES", "NO")

valid_classifications_full_log <- as.factor(classifications)
va_full_check_log = factor(va_full_y_log)


CM_full_log<- confusionMatrix(data = valid_classifications_full_log, #predictions
                      reference = va_full_check_log,
                      positive = "YES"#actuals
                      )
TPR_full_log <- as.numeric(CM_full_log$byClass["Sensitivity"])
FPR_full_log <- 1 - as.numeric(CM_full_log$byClass["Specificity"])
```

```{r logistic model training performance results + generalization performance results}
cat("Training performance of logistic model: TPR:", TPR_train_log, "\n")
cat("Training performance of logistic model: FPR:", FPR_train_log, "\n")
cat("Generalization performance of logistic model on validation data: TPR:", TPR_log, "\n")
cat("Generalization performance of logistic model on validation data: FPR:", FPR_log, "\n")
cat("Generalization performance of logistic model on holdout data: TPR:", TPR_full_log, "\n")
cat("Generalization performance of logistic model on holdout data: FPR:", FPR_full_log, "\n")
```

```{r amenities and host_verifications, echo = FALSE, warning = False}
##Test clean

split_verifications <- strsplit(as.character(test_clean$host_verifications), ",")
verifications_matrix <- sapply(unique_verifications, function(x) {
  sapply(split_verifications, function(y) { 
    as.numeric(x %in% y)
  })
})
length(verifications_matrix)

test_clean <- cbind(test_clean, verifications_matrix)
test_clean <- test_clean[, !names(test_clean) %in% "host_verifications"]

##Train clean

verifications_matrix <- sapply(unique_verifications, function(x) {
  sapply(split_ver, function(y) { 
    as.numeric(x %in% y)
  })
})
length(verifications_matrix)

train_clean <- cbind(train_clean, verifications_matrix)
train_clean <- train_clean[, !names(train_clean) %in% "host_verifications"]


## Code for amenities

## Code to make list of unique amenities
split_ver <- strsplit(as.character(train_clean$amenities), ",")
unique_amenities <- na.omit(unique(unlist(split_ver)))

##Test clean

split_amenities <- strsplit(as.character(test_clean$amenities), ",")
amenities_matrix <- sapply(unique_amenities, function(x) {
  sapply(split_amenities, function(y) { 
    as.numeric(x %in% y)
  })
})
length(amenities_matrix)

test_clean <- cbind(test_clean, amenities_matrix)
test_clean <- test_clean[, !names(test_clean) %in% "amenities"]

##Train clean

amenities_matrix <- sapply(unique_amenities, function(x) {
  sapply(split_ver, function(y) { 
    as.numeric(x %in% y)
  })
})
length(amenities_matrix)

train_clean <- cbind(train_clean, amenities_matrix)
train_clean <- train_clean[, !names(train_clean) %in% "amenities"]

columns_with_na <- colSums(is.na(train_clean)) > 0
```
```{r dropping non numeric columns}

train_clean <- train_clean%>%
   select(-c(   neighborhood_overview, access,
                transit, description, features, interaction,
                name, house_rules, space, square_feet, host_neighbourhood))

```

```{r 2nd model: ridge model, echo=FALSE, warning = False}

## Ridge Model
##train-test split (80%-20% split)
train_clean_ridge1 <- train_clean[1:82067, ]
valid_clean_ridge1 <- train_clean[82068:nrow(train_clean), ]
train_clean_ridge <- as.matrix(train_clean[1:82067, ])
valid_clean_ridge <- as.matrix(train_clean[82068:nrow(train_clean), ])

va_inds_ridge <- sample(nrow(train_clean_ridge1), .2*nrow(train_clean_ridge1))

tr_ridge <- as.matrix(train_clean_ridge[-va_inds_ridge,])
va_ridge <- as.matrix(train_clean_ridge[va_inds_ridge,])

tr_y_ridge <- as.matrix(train_clean_ridge1[-va_inds_ridge,]$perfect_rating_score)
va_y_ridge <- as.matrix(train_clean_ridge1[va_inds_ridge,]$perfect_rating_score)

tr_full_y_ridge <- as.matrix(train_clean_ridge1$perfect_rating_score)
va_full_y_ridge <- as.matrix(valid_clean_ridge1$perfect_rating_score)


grid <- 10^ seq(-5, 5, length.out = 100)

ridge_model <- cv.glmnet(tr_ridge, as.factor(tr_y_ridge), alpha = 0, lambda = grid, family = "binomial")

#plot(ridge_model)

# Report the optimal lambda
optimal_lambda <- ridge_model$lambda.min
cat("Optimal lambda:", optimal_lambda)

ridge_model_optimal <- glmnet(tr_ridge, as.factor(tr_y_ridge), alpha = 0, lambda = optimal_lambda, family = "binomial")

##Training performance

preds_ridge <- predict(ridge_model_optimal, newx = tr_ridge ,type="response")
class_preds <- ifelse(preds_ridge > 0.5, "YES", "NO")

# Make a variable importance plot
vip(ridge_model_optimal, num_features = 20)

## CM ussing text+numeric
valid_classifications_train_ridge <- as.factor(class_preds)
tr_check_ridge = factor(tr_y_ridge)


CM_train_ridge<- confusionMatrix(data = valid_classifications_train_ridge, #predictions
                      reference = tr_check_ridge,
                      positive = "YES"#actuals
                      )
TPR_train_ridge <- as.numeric(CM_train_ridge$byClass["Sensitivity"])
FPR_train_ridge <- 1 - as.numeric(CM_train_ridge$byClass["Specificity"])

##Generalization performance

preds_ridge <- predict(ridge_model_optimal, newx = va_ridge, type = "response")
valid_classifications_valid_ridge <- ifelse(preds_ridge > .5, "YES", "NO")

valid_classifications_ridge <- as.factor(valid_classifications_valid_ridge)
va_check_ridge = factor(va_y_ridge)


CM_ridge<- confusionMatrix(data = valid_classifications_ridge, #predictions
                      reference = va_check_ridge,
                      positive = "YES"#actuals
                      )
TPR_ridge <- as.numeric(CM_ridge$byClass["Sensitivity"])
FPR_ridge <- 1 - as.numeric(CM_ridge$byClass["Specificity"])

##Holdout performance

preds <- predict(ridge_model_optimal, newx = valid_clean_ridge, type = "response")
classifications <- ifelse(preds > .5, "YES", "NO")

valid_classifications_full_ridge <- as.factor(classifications)
va_full_check_ridge = factor(va_full_y_ridge)


CM_full_ridge<- confusionMatrix(data = valid_classifications_full_ridge, #predictions
                      reference = va_full_check_ridge,
                      positive = "YES"#actuals
                      )
TPR_full_ridge <- as.numeric(CM_full_ridge$byClass["Sensitivity"])
FPR_full_ridge <- 1 - as.numeric(CM_full_ridge$byClass["Specificity"])
```
```{r ridge model training performance results + generalization performance results}
cat("Training performance of ridge model: TPR:", TPR_train_ridge, "\n")
cat("Training performance of ridge model: FPR:", FPR_train_ridge, "\n")
cat("Generalization performance of ridge model on validation data: TPR:", TPR_ridge, "\n")
cat("Generalization performance of ridge model on validation data: FPR:", FPR_ridge, "\n")
cat("Generalization performance of ridge model on holdout data: TPR:", TPR_full_ridge, "\n")
cat("Generalization performance of ridge model on holdout data: FPR:", FPR_full_ridge, "\n")
```

```{r ridge model fitting curve}
# Fit ridge model with different lambda values
lambda_seq <- 10^ seq(-5, 5, length.out = 100)
ridge_model <- cv.glmnet(tr_ridge, as.factor(tr_y_ridge), alpha = 0, lambda = lambda_seq, family = "binomial")

# Plot the fitting curve
plot(ridge_model$glmnet.fit, xvar = "lambda", label = TRUE, xlim=c(-5, -2))
```


```{r 3rd model: lasso model, echo=FALSE, warning = False}
## Lasso Model
##train-test split (80%-20% split)
train_clean_lasso1 <- train_clean[1:82067, ]
valid_clean_lasso1 <- train_clean[82068:nrow(train_clean), ]
train_clean_lasso <- as.matrix(train_clean[1:82067, ])
valid_clean_lasso <- as.matrix(train_clean[82068:nrow(train_clean), ])

va_inds_lasso <- sample(nrow(train_clean_lasso1), .2*nrow(train_clean_lasso1))

tr_lasso <- as.matrix(train_clean_lasso[-va_inds_lasso,])
va_lasso <- as.matrix(train_clean_lasso[va_inds_lasso,])

tr_y_lasso <- as.matrix(train_clean_lasso1[-va_inds_lasso,]$perfect_rating_score)
va_y_lasso <- as.matrix(train_clean_lasso1[va_inds_lasso,]$perfect_rating_score)

tr_full_y_lasso <- as.matrix(train_clean_lasso1$perfect_rating_score)
va_full_y_lasso <- as.matrix(valid_clean_lasso1$perfect_rating_score)


grid <- 10^ seq(-5, 5, length.out = 100)

lasso_model <- cv.glmnet(tr_lasso, as.factor(tr_y_lasso), alpha = 1, lambda = grid, family = "binomial")

# Report the optimal lambda
optimal_lambda <- lasso_model$lambda.min
cat("Optimal lambda:", optimal_lambda)

lasso_model_optimal <- glmnet(tr_lasso, as.factor(tr_y_lasso), alpha = 1, lambda = optimal_lambda, family = "binomial")

##Training performance

preds_lasso <- predict(lasso_model_optimal, newx = tr_lasso ,type="response")
class_preds <- ifelse(preds_lasso > 0.5, "YES", "NO")

# Make a variable importance plot
vip(lasso_model_optimal, num_features = 20)

## CM ussing text+numeric
valid_classifications_train_lasso <- as.factor(class_preds)
tr_check_lasso = factor(tr_y_lasso)


CM_train_lasso<- confusionMatrix(data = valid_classifications_train_lasso, #predictions
                      reference = tr_check_lasso,
                      positive = "YES"#actuals
                      )
TPR_train_lasso <- as.numeric(CM_train_lasso$byClass["Sensitivity"])
FPR_train_lasso <- 1 - as.numeric(CM_train_lasso$byClass["Specificity"])

##Generalization performance

preds_lasso <- predict(lasso_model_optimal, newx = va_lasso, type = "response")
valid_classifications_valid_lasso <- ifelse(preds_lasso > .5, "YES", "NO")

valid_classifications_lasso <- as.factor(valid_classifications_valid_lasso)
va_check_lasso = factor(va_y_lasso)


CM_lasso<- confusionMatrix(data = valid_classifications_lasso, #predictions
                      reference = va_check_lasso,
                      positive = "YES"#actuals
                      )
TPR_lasso <- as.numeric(CM_lasso$byClass["Sensitivity"])
FPR_lasso <- 1 - as.numeric(CM_lasso$byClass["Specificity"])

##Holdout performance
preds <- predict(lasso_model_optimal, newx = valid_clean_lasso, type = "response")
classifications <- ifelse(preds > .5, "YES", "NO")

valid_classifications_full_lasso <- as.factor(classifications)
va_full_check_lasso = factor(va_full_y_lasso)


CM_full_lasso<- confusionMatrix(data = valid_classifications_full_lasso, #predictions
                      reference = va_full_check_lasso,
                      positive = "YES"#actuals
                      )
TPR_full_lasso <- as.numeric(CM_full_lasso$byClass["Sensitivity"])
FPR_full_lasso <- 1 - as.numeric(CM_full_lasso$byClass["Specificity"])
```


```{r lasso model training performance results + generalization performance results}
cat("Training performance of lasso model: TPR:", TPR_train_lasso, "\n")
cat("Training performance of lasso model: FPR:", FPR_train_lasso, "\n")
cat("Generalization performance of lasso model on validation data: TPR:", TPR_lasso, "\n")
cat("Generalization performance of lasso model on validation data: FPR:", FPR_lasso, "\n")
cat("Generalization performance of lasso model on holdout data: TPR:", TPR_full_lasso, "\n")
cat("Generalization performance of lasso model on holdout data: FPR:", FPR_full_lasso, "\n")
```


```{r lasso model fitting curve}

# Fit LASSO model with different lambda values
par(plt=c(0.2, 1, 0.2, 1))  

tr_lasso_first_100 <- tr_lasso[1:15, ]
tr_y_lasso_first_100 <- tr_y_lasso[1:15, ]

# Fit LASSO model with different lambda values
lambda_seq <- 10^seq(0, -4, length = 5)  
lasso_model <- cv.glmnet(tr_lasso_first_100, as.factor(tr_y_lasso_first_100), alpha = 1, lambda = lambda_seq, family = "binomial")

# Plot the fitting curve
plot(lasso_model$glmnet.fit, xvar = "lambda", label = TRUE, ylim = c(-3,3), xlim = c(-7,0))

```

```{r 4th model: decision trees, echo= False, warning = False}
## Model
##train-test split (80%-20% split)
library(tree)
#tr_tree$perfect_rating_score
train_clean_tree <- train_clean[1:82067, ]
valid_clean_tree <- train_clean[82068:nrow(train_clean), ] 
va_inds_tree <- sample(nrow(train_clean_tree), .2*nrow(train_clean_tree))
tr_tree <- train_clean_tree[-va_inds_tree,]
va_tree <- train_clean_tree[va_inds_tree,]
tr_y_tree <- tr_tree$perfect_rating_score
va_y_tree <- va_tree$perfect_rating_score

tr_full_y_tree <- train_clean_tree$perfect_rating_score
va_full_y_tree <- valid_clean_tree$perfect_rating_score


default_tree <- tree(perfect_rating_score~., data = data.frame(tr_tree))


pruned_tree_5=prune.tree(default_tree, best = 3)

#Training performance
tree_preds <- predict(pruned_tree_5,newdata= data.frame(tr_tree))
tree_probs=tree_preds[,2]

classifications <- ifelse(tree_probs > .325, "YES", "NO")

valid_classifications_train_tree <- as.factor(classifications)
tr_check_tree = factor(tr_y_tree)


CM_train_tree<- confusionMatrix(data = valid_classifications_train_tree, #predictions
                      reference = tr_check_tree,
                      positive = "YES"#actuals
                      )
TPR_train_tree <- as.numeric(CM_train_tree$byClass["Sensitivity"])
FPR_train_tree <- 1 - as.numeric(CM_train_tree$byClass["Specificity"])

##Generalization performance

tree_preds_1 <- predict(pruned_tree_5,newdata= data.frame(va_tree))
tree_probs_1 <- tree_preds_1[,2]

classifications <- ifelse(tree_probs_1 > .325, "YES", "NO")
valid_classifications_tree <- as.factor(classifications)
va_check_tree = factor(va_y_tree)


CM_tree<- confusionMatrix(data = valid_classifications_tree, #predictions
                      reference = va_check_tree,
                      positive = "YES"#actuals
                      )
TPR_tree <- as.numeric(CM_tree$byClass["Sensitivity"])
FPR_tree <- 1 - as.numeric(CM_tree$byClass["Specificity"])

##Holdout performance
tree_preds_2 <- predict(pruned_tree_10,newdata= data.frame(valid_clean_tree))
tree_probs_2 <- tree_preds_2[,2]

classifications <- ifelse(tree_probs_2 > .325, "YES", "NO")

valid_classifications_full_tree <- as.factor(classifications)
va_full_check_tree = factor(va_full_y_tree)


CM_full_tree<- confusionMatrix(data = valid_classifications_full_tree, #predictions
                      reference = va_full_check_tree,
                      positive = "YES"#actuals
                      )
TPR_full_tree <- as.numeric(CM_full_tree$byClass["Sensitivity"])
FPR_full_tree <- 1 - as.numeric(CM_full_tree$byClass["Specificity"])
```

```{r decision tree training performance results + generalization performance results}
cat("Training performance of decision tree model: TPR:", TPR_train_tree, "\n")
cat("Training performance of decision tree model: FPR:", FPR_train_tree, "\n")
cat("Generalization performance of decision tree model on validation data: TPR:", TPR_tree, "\n")
cat("Generalization performance of decision tree model on validation data: FPR:", FPR_tree, "\n")
cat("Generalization performance of decision tree model on holdout data: TPR:", TPR_full_tree, "\n")
cat("Generalization performance of decision tree model on holdout data: FPR:", FPR_full_tree, "\n")
```
``` {r 5th model: random forest}
train_clean_rf <- train_clean[1:82067, ]
valid_clean_rf <- train_clean[82068:nrow(train_clean), ] 
##train-test split (80%-20% split)
va_inds_rf <- sample(nrow(train_clean_rf), .2*nrow(train_clean_rf))

tr_rf <- train_clean_rf[-va_inds_rf,]
va_rf <- train_clean_rf[va_inds_rf,]

tr_y_rf <- tr_rf$perfect_rating_score
va_y_rf <- va_rf$perfect_rating_score

tr_full_y_rf <- train_clean_rf$perfect_rating_score
va_full_y_rf <- valid_clean_rf$perfect_rating_score
```

```{r text_mining_train, echo = False, warning = False}
train_clean_rf <- train_clean_rf %>%
  mutate(
    transit = ifelse(is.na(transit), "Nonegiven", transit),
    description = ifelse(is.na(description), "Nonegiven", description),
    access = ifelse(is.na(access), "Nonegiven", access),
    neighborhood_overview = ifelse(is.na(neighborhood_overview), "Nonegiven", neighborhood_overview),
    interaction = ifelse(is.na(interaction), "Nonegiven", interaction),
    name = ifelse(is.na(name), "Nonegiven", name),
    house_rules = ifelse(is.na(house_rules), "Nonegiven", house_rules),
    space = ifelse(is.na(space), "Nonegiven", space),
    features = ifelse(is.na(features), "Nonegiven", features),
    id = row_number()) 

cleaning_tokenizer <- function(v) {
  v %>%
    removeNumbers %>% #remove all numbers
    removePunctuation %>% #remove all punctuation
    removeWords(tm::stopwords(kind="en")) %>% #remove stopwords
    stemDocument %>%
    word_tokenizer 
}

it_train_transit = itoken(train_clean_rf$transit, 
                          preprocessor = tolower, #preprocessing by converting to lowercase
                          tokenizer = cleaning_tokenizer, 
                          ids = train_clean_rf$id, 
                          progressbar = FALSE)


it_train_desc = itoken(train_clean_rf$description, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = train_clean_rf$id, 
                       progressbar = FALSE)

it_train_access = itoken(train_clean_rf$access, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = train_clean_rf$id, 
                         progressbar = FALSE)

it_train_neigh = itoken(train_clean_rf$neighborhood_overview, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean_rf$id, 
                        progressbar = FALSE)


it_train_interaction = itoken(train_clean_rf$interaction, 
                              preprocessor = tolower, #preprocessing by converting to lowercase
                              tokenizer = cleaning_tokenizer, 
                              ids = train_clean_rf$id, 
                              progressbar = FALSE)


it_train_name = itoken(train_clean_rf$name, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = train_clean_rf$id, 
                       progressbar = FALSE)

it_train_house_rules = itoken(train_clean_rf$house_rules, 
                              preprocessor = tolower, #preprocessing by converting to lowercase
                              tokenizer = cleaning_tokenizer, 
                              ids = train_clean_rf$id, 
                              progressbar = FALSE)

it_train_space = itoken(train_clean_rf$space, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean_rf$id, 
                        progressbar = FALSE)

it_train_features = itoken(train_clean_rf$features, 
                           preprocessor = tolower, #preprocessing by converting to lowercase
                           tokenizer = cleaning_tokenizer, 
                           ids = train_clean_rf$id, 
                           progressbar = FALSE)

it_train_juris = itoken(train_clean_rf$jurisdiction_names, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean_rf$id, 
                        progressbar = FALSE)


it_train_hneigh = itoken(train_clean_rf$host_neighbourhood, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = train_clean_rf$id, 
                         progressbar = FALSE)


##Transit Vocab
stop_words_transit <- c("your", "s", "get", "also", "im", "go", "take")
vocab_transit <- create_vocabulary(it_train_transit, ngram = c(1L, 2L), stopwords = stop_words_transit)

vocab_transit_final = prune_vocabulary(vocab_transit, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_transit = vocab_vectorizer(vocab_transit_final)

##Description vocab
stop_words_desc <- c("also", "can", "us")
vocab_desc <- create_vocabulary(it_train_desc, ngram = c(1L, 2L), stopwords = stop_words_desc)

vocab_desc_final = prune_vocabulary(vocab_desc, doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_desc = vocab_vectorizer(vocab_desc_final)

##Neighborhood overview vocab
stop_words_neigh <- c("will", "can")
vocab_neigh <- create_vocabulary(it_train_neigh, ngram = c(1L, 2L), stopwords = stop_words_neigh)

vocab_neigh_final = prune_vocabulary(vocab_neigh,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_neigh = vocab_vectorizer(vocab_neigh_final)

## Access vocab
stop_words_access <- c("also", "can")
vocab_access <- create_vocabulary(it_train_access, ngram = c(1L, 2L), stopwords = stop_words_access)

vocab_access_final = prune_vocabulary(vocab_access,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_access = vocab_vectorizer(vocab_access_final)

stop_words_interaction <- c("your", "s", "get", "also", "im", "go", "take")
vocab_interaction <- create_vocabulary(it_train_interaction, ngram = c(1L, 2L), stopwords = stop_words_interaction)

vocab_interaction_final = prune_vocabulary(vocab_interaction, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_interaction = vocab_vectorizer(vocab_interaction_final)

## Name vocab
stop_words_name <- c( "the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_name <- create_vocabulary(it_train_name, ngram = c(1L, 2L), stopwords = stop_words_name)

vocab_name_final = prune_vocabulary(vocab_name, doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_name = vocab_vectorizer(vocab_name_final)

## House Rules vocab
stop_words_house_rules <- c("the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_house_rules <- create_vocabulary(it_train_house_rules, ngram = c(1L, 2L), stopwords = stop_words_house_rules)

vocab_house_rules_final = prune_vocabulary(vocab_house_rules,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_house_rules = vocab_vectorizer(vocab_house_rules_final)

## Space vocab
stop_words_space <- c("the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_space <- create_vocabulary(it_train_space, ngram = c(1L, 2L), stopwords = stop_words_space)

vocab_space_final = prune_vocabulary(vocab_space,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_space = vocab_vectorizer(vocab_space_final)

#features vocab
stop_words_features <- c("host", "is", "has", "with","from")
vocab_features <- create_vocabulary(it_train_features, ngram = c(1L, 2L), stopwords = stop_words_features)

vocab_features_final = prune_vocabulary(vocab_features, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_features = vocab_vectorizer(vocab_features_final)

##Jurisdiction Names vocab
stop_words_juris <- c("city", "state")
vocab_juris <- create_vocabulary(it_train_juris, ngram = c(1L, 2L), stopwords = stop_words_juris)

vocab_juris_final = prune_vocabulary(vocab_juris,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_juris = vocab_vectorizer(vocab_juris_final)

##Host Neighborhood vocab

vocab_hneigh <- create_vocabulary(it_train_hneigh, ngram = c(1L, 2L), stopwords = stop_words_juris)

vocab_hneigh_final = prune_vocabulary(vocab_hneigh,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_hneigh = vocab_vectorizer(vocab_hneigh_final)

# Convert the training documents into a DTM
dtm_train_transit = create_dtm(it_train_transit, vectorizer_transit)
dtm_train_desc = create_dtm(it_train_desc, vectorizer_desc)
dtm_train_neigh = create_dtm(it_train_neigh, vectorizer_neigh)
dtm_train_access = create_dtm(it_train_access, vectorizer_access)
dtm_train_interaction = create_dtm(it_train_interaction, vectorizer_interaction)
dtm_train_name = create_dtm(it_train_name, vectorizer_name)
dtm_train_house_rules = create_dtm(it_train_house_rules, vectorizer_house_rules)
dtm_train_space = create_dtm(it_train_space, vectorizer_space)
dtm_train_features = create_dtm(it_train_features, vectorizer_features)
dtm_train_juris = create_dtm(it_train_juris, vectorizer_juris)
dtm_train_hneigh = create_dtm(it_train_hneigh, vectorizer_hneigh)

dtm_train_bin <- cbind(dtm_train_access, dtm_train_neigh, dtm_train_desc, dtm_train_transit,
                       dtm_train_interaction, dtm_train_name, dtm_train_house_rules, dtm_train_space,
                       dtm_train_features, dtm_train_juris, dtm_train_hneigh)
# Make a TFIDF DTM
tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(dtm_train_bin, tfidf)
dtm_train_tfidf_dense <- as.matrix(dtm_train_tfidf)

# Convert regular matrix to dataframe
dtm_train_tfidf_df_1 <- as.data.frame(dtm_train_tfidf_dense)

mean_tfidf <- colMeans(dtm_train_tfidf_df_1)

# Sort mean TF-IDF scores in descending order
sorted_mean_tfidf <- sort(mean_tfidf, decreasing = TRUE)

# Select top terms with highest mean TF-IDF scores
top_terms <- head(sorted_mean_tfidf, 50) 

# Get indices of top terms
top_term_indices <- match(names(sorted_mean_tfidf), names(mean_tfidf))[1:length(top_terms)]

# Filter DTM to keep only columns of top terms
dtm_train_tfidf_df<- dtm_train_tfidf_df_1[, top_term_indices]
```


```{r text_mining_test, echo = False, warning = False}
# Text mining for test

valid_clean_rf <- valid_clean_rf %>%
  mutate(
    transit = ifelse(is.na(transit), "Nonegiven", transit),
    description = ifelse(is.na(description), "Nonegiven", description),
    access = ifelse(is.na(access), "Nonegiven", access),
    neighborhood_overview = ifelse(is.na(neighborhood_overview), "Nonegiven", neighborhood_overview),
    interaction = ifelse(is.na(interaction), "Nonegiven", interaction),
    name = ifelse(is.na(name), "Nonegiven", name),
    house_rules = ifelse(is.na(house_rules), "Nonegiven", house_rules),
    space = ifelse(is.na(space), "Nonegiven", space),
    features = ifelse(is.na(features), "Nonegiven", features),
    id = row_number()
  )

it_test_transit = itoken(valid_clean_rf$transit, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = valid_clean_rf$id, 
                         progressbar = FALSE)

it_test_desc = itoken(valid_clean_rf$description, 
                      preprocessor = tolower, #preprocessing by converting to lowercase
                      tokenizer = cleaning_tokenizer, 
                      ids = valid_clean_rf$id, 
                      progressbar = FALSE)

it_test_access = itoken(valid_clean_rf$access, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = valid_clean_rf$id, 
                        progressbar = FALSE)

it_test_neigh = itoken(valid_clean_rf$neighborhood_overview, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = valid_clean_rf$id, 
                       progressbar = FALSE)

it_test_interaction = itoken(valid_clean_rf$interaction, 
                             preprocessor = tolower, #preprocessing by converting to lowercase
                             tokenizer = cleaning_tokenizer, 
                             ids = valid_clean_rf$id, 
                             progressbar = FALSE)

it_test_name = itoken(valid_clean_rf$name, 
                      preprocessor = tolower, #preprocessing by converting to lowercase
                      tokenizer = cleaning_tokenizer, 
                      ids = valid_clean_rf$id, 
                      progressbar = FALSE)

it_test_house_rules = itoken(valid_clean_rf$house_rules, 
                             preprocessor = tolower, #preprocessing by converting to lowercase
                             tokenizer = cleaning_tokenizer, 
                             ids = valid_clean_rf$id, 
                             progressbar = FALSE)

it_test_space = itoken(valid_clean_rf$space, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = valid_clean_rf$id, 
                       progressbar = FALSE)

it_test_features = itoken(valid_clean_rf$features, 
                          preprocessor = tolower, #preprocessing by converting to lowercase
                          tokenizer = cleaning_tokenizer, 
                          ids = valid_clean_rf$id, 
                          progressbar = FALSE)

it_test_juris = itoken(valid_clean_rf$jurisdiction_names, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = valid_clean_rf$id, 
                       progressbar = FALSE)

it_test_hneigh = itoken(valid_clean_rf$host_neighbourhood, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = valid_clean_rf$id, 
                        progressbar = FALSE)


# Convert the test documents into a DTM
dtm_test_transit = create_dtm(it_test_transit, vectorizer_transit)
dtm_test_desc = create_dtm(it_test_desc, vectorizer_desc)
dtm_test_neigh = create_dtm(it_test_neigh, vectorizer_neigh)
dtm_test_access = create_dtm(it_test_access, vectorizer_access)
dtm_test_interaction = create_dtm(it_test_interaction, vectorizer_interaction)
dtm_test_name = create_dtm(it_test_name, vectorizer_name)
dtm_test_house_rules = create_dtm(it_test_house_rules, vectorizer_house_rules)
dtm_test_space = create_dtm(it_test_space, vectorizer_space)
dtm_test_features = create_dtm(it_test_features, vectorizer_features)
dtm_test_juris = create_dtm(it_test_juris, vectorizer_juris)
dtm_test_hneigh = create_dtm(it_test_hneigh, vectorizer_hneigh)

dtm_test_bin <- cbind(dtm_test_access, dtm_test_neigh, dtm_test_desc, dtm_test_transit,
                      dtm_test_interaction, dtm_test_name, dtm_test_house_rules, dtm_test_space,
                      dtm_test_features, dtm_test_juris, dtm_test_hneigh)

# Make a TFIDF DTM for test data
tfidf = TfIdf$new()
dtm_test_tfidf = fit_transform(dtm_test_bin, tfidf)
dtm_test_tfidf_dense <- as.matrix(dtm_test_tfidf)

# Convert regular matrix to dataframe for test data
dtm_test_tfidf_df_1 <- as.data.frame(dtm_test_tfidf_dense)

mean_tfidf <- colMeans(dtm_test_tfidf_df_1)

# Sort mean TF-IDF scores in descending order for test data
sorted_mean_tfidf <- sort(mean_tfidf, decreasing = TRUE)

# Select top terms with highest mean TF-IDF scores for test data
top_terms <- head(sorted_mean_tfidf, 50) 

# Get indices of top terms for test data
top_term_indices <- match(names(sorted_mean_tfidf), names(mean_tfidf))[1:length(top_terms)]

# Filter DTM to keep only columns of top terms for test data
dtm_test_tfidf_df <- dtm_test_tfidf_df_1[, top_term_indices]
```

```{r X_Train X_test, echo = False, warning = False}

names(tr_rf) <- gsub("_", "1", names(train_clean), fixed = TRUE)
names(va_rf) <- gsub("_", "1", names(test_clean), fixed = TRUE)

non_numeric_cols <- sapply(train_clean_rf, function(x) !is.numeric(x))

# Exclude non-numeric columns
train_clean2 <- train_clean_rf[, !non_numeric_cols]

non_numeric_cols2 <- sapply(valid_clean_rf, function(x) !is.numeric(x))

# Exclude non-numeric columns
test_clean2 <- valid_clean_rf[, !non_numeric_cols2]

X_train <- cbind(as.matrix(train_clean2), as.matrix(dtm_train_tfidf_df))
X_test <- cbind(as.matrix(test_clean2), as.matrix(dtm_test_tfidf_df))
```


```{r random_forest_training, warning = False}

tr_x <- X_train[-va_inds_rf,]
va_x <- X_train[va_inds_rf,] 

common_cols <- intersect(colnames(tr_x), colnames(va_x))

# Reorder tr_x and va_x based on the common set of column names
tr_x <- tr_x[, common_cols]
va_x <- va_x[, common_cols]

rf_mod <- ranger(x = tr_x, y = as.factor(tr_y_rf),
                 mtry = 50, num.trees = 2500,
                 importance = "impurity", probability = TRUE)

# Predict probabilities of class 1 (YES) for the validation set
preds_rf <- predict(rf_mod, data = tr_x, type = "response")$predictions[, 2]

# Convert probabilities to class labels (YES or NO)
class_preds <- ifelse(preds_rf > 0.464, "YES", "NO")
valid_classifications <- as.factor(class_preds)

CM <- confusionMatrix(data = valid_classifications, #predictions
                      reference = tr_y_rf, #actuals
                      positive= "YES")
TPR <- as.numeric(CM$byClass["Sensitivity"])
TNR <- as.numeric(CM$byClass["Specificity"])

TN <- CM$table[1,1]
FP <- CM$table[2,1]

FPR <- TN/(TN+FP)
FPR <- 1-TNR

cat("TPR:", TPR)
cat("FPR:", FPR)
```

```{r random_forest_validation, warning= False}

# Predict probabilities of class 1 (YES) for the validation set
preds_rf <- predict(rf_mod, data = va_x, type = "response")$predictions[, 2]

# Convert probabilities to class labels (YES or NO)
class_preds <- ifelse(preds_rf > 0.464, "YES", "NO")
valid_classifications <- as.factor(class_preds)

CM <- confusionMatrix(data = valid_classifications, #predictions
                      reference = va_y_rf, #actuals
                      positive= "YES")
TPR <- as.numeric(CM$byClass["Sensitivity"])
TNR <- as.numeric(CM$byClass["Specificity"])

TN <- CM$table[1,1]
FP <- CM$table[2,1]

FPR <- TN/(TN+FP)
FPR <- 1-TNR

cat("TPR:", TPR)
cat("FPR:", FPR)
```


```{r random_forest_holdout, warning= False}

tr_x <- X_train[-va_inds_rf,]
va_x <- X_test

common_cols <- intersect(colnames(tr_x), colnames(va_x))

# Reorder tr_x and va_x based on the common set of column names
tr_x <- tr_x[, common_cols]
va_x <- va_x[, common_cols]

rf_mod <- ranger(x = tr_x, y = as.factor(tr_y_rf),
                 mtry = 50, num.trees = 2500,
                 importance = "impurity", probability = TRUE)

# Predict probabilities of class 1 (YES) for the validation set
preds_rf <- predict(rf_mod, data = va_x, type = "response")$predictions[, 2]

# Convert probabilities to class labels (YES or NO)
class_preds <- ifelse(preds_rf > 0.464, "YES", "NO")
valid_classifications <- as.factor(class_preds)

CM <- confusionMatrix(data = valid_classifications, #predictions
                      reference = va_full_y_rf, #actuals
                      positive= "YES")
TPR <- as.numeric(CM$byClass["Sensitivity"])
TNR <- as.numeric(CM$byClass["Specificity"])

TN <- CM$table[1,1]
FP <- CM$table[2,1]

FPR <- TN/(TN+FP)
FPR <- 1-TNR

cat("TPR:", TPR)
cat("FPR:", FPR)
```

```{r text_mining_train_final}
train_clean <- train_clean %>%
  mutate(
    transit = ifelse(is.na(transit), "Nonegiven", transit),
    description = ifelse(is.na(description), "Nonegiven", description),
    access = ifelse(is.na(access), "Nonegiven", access),
    neighborhood_overview = ifelse(is.na(neighborhood_overview), "Nonegiven", neighborhood_overview),
    interaction = ifelse(is.na(interaction), "Nonegiven", interaction),
    name = ifelse(is.na(name), "Nonegiven", name),
    house_rules = ifelse(is.na(house_rules), "Nonegiven", house_rules),
    space = ifelse(is.na(space), "Nonegiven", space),
    features = ifelse(is.na(features), "Nonegiven", features),
    id = row_number()) 

cleaning_tokenizer <- function(v) {
  v %>%
    removeNumbers %>% #remove all numbers
    removePunctuation %>% #remove all punctuation
    removeWords(tm::stopwords(kind="en")) %>% #remove stopwords
    stemDocument %>%
    word_tokenizer 
}

it_train_transit = itoken(train_clean$transit, 
                          preprocessor = tolower, #preprocessing by converting to lowercase
                          tokenizer = cleaning_tokenizer, 
                          ids = train_clean$id, 
                          progressbar = FALSE)


it_train_desc = itoken(train_clean$description, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = train_clean$id, 
                       progressbar = FALSE)

it_train_access = itoken(train_clean$access, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = train_clean$id, 
                         progressbar = FALSE)

it_train_neigh = itoken(train_clean$neighborhood_overview, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean$id, 
                        progressbar = FALSE)


it_train_interaction = itoken(train_clean$interaction, 
                              preprocessor = tolower, #preprocessing by converting to lowercase
                              tokenizer = cleaning_tokenizer, 
                              ids = train_clean$id, 
                              progressbar = FALSE)


it_train_name = itoken(train_clean$name, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = train_clean$id, 
                       progressbar = FALSE)

it_train_house_rules = itoken(train_clean$house_rules, 
                              preprocessor = tolower, #preprocessing by converting to lowercase
                              tokenizer = cleaning_tokenizer, 
                              ids = train_clean$id, 
                              progressbar = FALSE)

it_train_space = itoken(train_clean$space, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean$id, 
                        progressbar = FALSE)

it_train_features = itoken(train_clean$features, 
                           preprocessor = tolower, #preprocessing by converting to lowercase
                           tokenizer = cleaning_tokenizer, 
                           ids = train_clean$id, 
                           progressbar = FALSE)

it_train_juris = itoken(train_clean$jurisdiction_names, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = train_clean$id, 
                        progressbar = FALSE)


it_train_hneigh = itoken(train_clean$host_neighbourhood, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = train_clean$id, 
                         progressbar = FALSE)


##Transit Vocab
stop_words_transit <- c("your", "s", "get", "also", "im", "go", "take")
vocab_transit <- create_vocabulary(it_train_transit, ngram = c(1L, 2L), stopwords = stop_words_transit)

vocab_transit_final = prune_vocabulary(vocab_transit, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_transit = vocab_vectorizer(vocab_transit_final)

##Description vocab
stop_words_desc <- c("also", "can", "us")
vocab_desc <- create_vocabulary(it_train_desc, ngram = c(1L, 2L), stopwords = stop_words_desc)

vocab_desc_final = prune_vocabulary(vocab_desc, doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_desc = vocab_vectorizer(vocab_desc_final)

##Neighborhood overview vocab
stop_words_neigh <- c("will", "can")
vocab_neigh <- create_vocabulary(it_train_neigh, ngram = c(1L, 2L), stopwords = stop_words_neigh)

vocab_neigh_final = prune_vocabulary(vocab_neigh,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_neigh = vocab_vectorizer(vocab_neigh_final)

## Access vocab
stop_words_access <- c("also", "can")
vocab_access <- create_vocabulary(it_train_access, ngram = c(1L, 2L), stopwords = stop_words_access)

vocab_access_final = prune_vocabulary(vocab_access,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_access = vocab_vectorizer(vocab_access_final)

stop_words_interaction <- c("your", "s", "get", "also", "im", "go", "take")
vocab_interaction <- create_vocabulary(it_train_interaction, ngram = c(1L, 2L), stopwords = stop_words_interaction)

vocab_interaction_final = prune_vocabulary(vocab_interaction, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_interaction = vocab_vectorizer(vocab_interaction_final)

## Name vocab
stop_words_name <- c( "the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_name <- create_vocabulary(it_train_name, ngram = c(1L, 2L), stopwords = stop_words_name)

vocab_name_final = prune_vocabulary(vocab_name, doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_name = vocab_vectorizer(vocab_name_final)

## House Rules vocab
stop_words_house_rules <- c("the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_house_rules <- create_vocabulary(it_train_house_rules, ngram = c(1L, 2L), stopwords = stop_words_house_rules)

vocab_house_rules_final = prune_vocabulary(vocab_house_rules,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_house_rules = vocab_vectorizer(vocab_house_rules_final)

## Space vocab
stop_words_space <- c("the", "a", "an", "and", "of", "in", "on", "at", "with", "for")
vocab_space <- create_vocabulary(it_train_space, ngram = c(1L, 2L), stopwords = stop_words_space)

vocab_space_final = prune_vocabulary(vocab_space,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_space = vocab_vectorizer(vocab_space_final)

#features vocab
stop_words_features <- c("host", "is", "has", "with","from")
vocab_features <- create_vocabulary(it_train_features, ngram = c(1L, 2L), stopwords = stop_words_features)

vocab_features_final = prune_vocabulary(vocab_features, doc_proportion_max = 0.8, doc_proportion_min = 0.02)
vectorizer_features = vocab_vectorizer(vocab_features_final)

##Jurisdiction Names vocab
stop_words_juris <- c("city", "state")
vocab_juris <- create_vocabulary(it_train_juris, ngram = c(1L, 2L), stopwords = stop_words_juris)

vocab_juris_final = prune_vocabulary(vocab_juris,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_juris = vocab_vectorizer(vocab_juris_final)

##Host Neighborhood vocab

vocab_hneigh <- create_vocabulary(it_train_hneigh, ngram = c(1L, 2L), stopwords = stop_words_juris)

vocab_hneigh_final = prune_vocabulary(vocab_hneigh,  doc_proportion_max = 0.8, doc_proportion_min = 0.02)

vectorizer_hneigh = vocab_vectorizer(vocab_hneigh_final)

# Convert the training documents into a DTM
dtm_train_transit = create_dtm(it_train_transit, vectorizer_transit)
dtm_train_desc = create_dtm(it_train_desc, vectorizer_desc)
dtm_train_neigh = create_dtm(it_train_neigh, vectorizer_neigh)
dtm_train_access = create_dtm(it_train_access, vectorizer_access)
dtm_train_interaction = create_dtm(it_train_interaction, vectorizer_interaction)
dtm_train_name = create_dtm(it_train_name, vectorizer_name)
dtm_train_house_rules = create_dtm(it_train_house_rules, vectorizer_house_rules)
dtm_train_space = create_dtm(it_train_space, vectorizer_space)
dtm_train_features = create_dtm(it_train_features, vectorizer_features)
dtm_train_juris = create_dtm(it_train_juris, vectorizer_juris)
dtm_train_hneigh = create_dtm(it_train_hneigh, vectorizer_hneigh)

dtm_train_bin <- cbind(dtm_train_access, dtm_train_neigh, dtm_train_desc, dtm_train_transit,
                       dtm_train_interaction, dtm_train_name, dtm_train_house_rules, dtm_train_space,
                       dtm_train_features, dtm_train_juris, dtm_train_hneigh)
# Make a TFIDF DTM
tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(dtm_train_bin, tfidf)
dtm_train_tfidf_dense <- as.matrix(dtm_train_tfidf)

# Convert regular matrix to dataframe
dtm_train_tfidf_df_1 <- as.data.frame(dtm_train_tfidf_dense)

mean_tfidf <- colMeans(dtm_train_tfidf_df_1)

# Sort mean TF-IDF scores in descending order
sorted_mean_tfidf <- sort(mean_tfidf, decreasing = TRUE)

# Select top terms with highest mean TF-IDF scores
top_terms <- head(sorted_mean_tfidf, 50) 

# Get indices of top terms
top_term_indices <- match(names(sorted_mean_tfidf), names(mean_tfidf))[1:length(top_terms)]

# Filter DTM to keep only columns of top terms
dtm_train_tfidf_df<- dtm_train_tfidf_df_1[, top_term_indices]
```

```{r text_mining_test_final}
# Text mining for test

test_clean <- test_clean %>%
  mutate(
    transit = ifelse(is.na(transit), "Nonegiven", transit),
    description = ifelse(is.na(description), "Nonegiven", description),
    access = ifelse(is.na(access), "Nonegiven", access),
    neighborhood_overview = ifelse(is.na(neighborhood_overview), "Nonegiven", neighborhood_overview),
    interaction = ifelse(is.na(interaction), "Nonegiven", interaction),
    name = ifelse(is.na(name), "Nonegiven", name),
    house_rules = ifelse(is.na(house_rules), "Nonegiven", house_rules),
    space = ifelse(is.na(space), "Nonegiven", space),
    features = ifelse(is.na(features), "Nonegiven", features),
    id = row_number()
  )

it_test_transit = itoken(test_clean$transit, 
                         preprocessor = tolower, #preprocessing by converting to lowercase
                         tokenizer = cleaning_tokenizer, 
                         ids = test_clean$id, 
                         progressbar = FALSE)

it_test_desc = itoken(test_clean$description, 
                      preprocessor = tolower, #preprocessing by converting to lowercase
                      tokenizer = cleaning_tokenizer, 
                      ids = test_clean$id, 
                      progressbar = FALSE)

it_test_access = itoken(test_clean$access, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = test_clean$id, 
                        progressbar = FALSE)

it_test_neigh = itoken(test_clean$neighborhood_overview, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = test_clean$id, 
                       progressbar = FALSE)

it_test_interaction = itoken(test_clean$interaction, 
                             preprocessor = tolower, #preprocessing by converting to lowercase
                             tokenizer = cleaning_tokenizer, 
                             ids = test_clean$id, 
                             progressbar = FALSE)

it_test_name = itoken(test_clean$name, 
                      preprocessor = tolower, #preprocessing by converting to lowercase
                      tokenizer = cleaning_tokenizer, 
                      ids = test_clean$id, 
                      progressbar = FALSE)

it_test_house_rules = itoken(test_clean$house_rules, 
                             preprocessor = tolower, #preprocessing by converting to lowercase
                             tokenizer = cleaning_tokenizer, 
                             ids = test_clean$id, 
                             progressbar = FALSE)

it_test_space = itoken(test_clean$space, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = test_clean$id, 
                       progressbar = FALSE)

it_test_features = itoken(test_clean$features, 
                          preprocessor = tolower, #preprocessing by converting to lowercase
                          tokenizer = cleaning_tokenizer, 
                          ids = test_clean$id, 
                          progressbar = FALSE)

it_test_juris = itoken(test_clean$jurisdiction_names, 
                       preprocessor = tolower, #preprocessing by converting to lowercase
                       tokenizer = cleaning_tokenizer, 
                       ids = test_clean$id, 
                       progressbar = FALSE)

it_test_hneigh = itoken(test_clean$host_neighbourhood, 
                        preprocessor = tolower, #preprocessing by converting to lowercase
                        tokenizer = cleaning_tokenizer, 
                        ids = test_clean$id, 
                        progressbar = FALSE)


# Convert the test documents into a DTM
dtm_test_transit = create_dtm(it_test_transit, vectorizer_transit)
dtm_test_desc = create_dtm(it_test_desc, vectorizer_desc)
dtm_test_neigh = create_dtm(it_test_neigh, vectorizer_neigh)
dtm_test_access = create_dtm(it_test_access, vectorizer_access)
dtm_test_interaction = create_dtm(it_test_interaction, vectorizer_interaction)
dtm_test_name = create_dtm(it_test_name, vectorizer_name)
dtm_test_house_rules = create_dtm(it_test_house_rules, vectorizer_house_rules)
dtm_test_space = create_dtm(it_test_space, vectorizer_space)
dtm_test_features = create_dtm(it_test_features, vectorizer_features)
dtm_test_juris = create_dtm(it_test_juris, vectorizer_juris)
dtm_test_hneigh = create_dtm(it_test_hneigh, vectorizer_hneigh)

dtm_test_bin <- cbind(dtm_test_access, dtm_test_neigh, dtm_test_desc, dtm_test_transit,
                      dtm_test_interaction, dtm_test_name, dtm_test_house_rules, dtm_test_space,
                      dtm_test_features, dtm_test_juris, dtm_test_hneigh)

# Make a TFIDF DTM for test data
tfidf = TfIdf$new()
dtm_test_tfidf = fit_transform(dtm_test_bin, tfidf)
dtm_test_tfidf_dense <- as.matrix(dtm_test_tfidf)

# Convert regular matrix to dataframe for test data
dtm_test_tfidf_df_1 <- as.data.frame(dtm_test_tfidf_dense)

mean_tfidf <- colMeans(dtm_test_tfidf_df_1)

# Sort mean TF-IDF scores in descending order for test data
sorted_mean_tfidf <- sort(mean_tfidf, decreasing = TRUE)

# Select top terms with highest mean TF-IDF scores for test data
top_terms <- head(sorted_mean_tfidf, 50) 

# Get indices of top terms for test data
top_term_indices <- match(names(sorted_mean_tfidf), names(mean_tfidf))[1:length(top_terms)]

# Filter DTM to keep only columns of top terms for test data
dtm_test_tfidf_df <- dtm_test_tfidf_df_1[, top_term_indices]
```

```{r X_Train, X_test final}

names(train_clean) <- gsub("_", "1", names(train_clean), fixed = TRUE)
names(test_clean) <- gsub("_", "1", names(test_clean), fixed = TRUE)

non_numeric_cols <- sapply(train_clean, function(x) !is.numeric(x))

# Exclude non-numeric columns
train_clean2 <- train_clean[, !non_numeric_cols]

non_numeric_cols2 <- sapply(test_clean, function(x) !is.numeric(x))

# Exclude non-numeric columns
test_clean2 <- test_clean[, !non_numeric_cols2]

X_train <- cbind(as.matrix(train_clean2), as.matrix(dtm_train_tfidf_df))
X_test <- cbind(as.matrix(test_clean2), as.matrix(dtm_test_tfidf_df))
```

```{r random_forest_training final}
tr_y_full <- train_clean$perfect_rating_score

tr_x <- X_train
va_x <- X_test
length(va_x)
common_cols <- intersect(colnames(tr_x), colnames(va_x))

# Reorder tr_x and va_x based on the common set of column names
tr_x <- tr_x[, common_cols]
va_x <- va_x[, common_cols]

rf_mod <- ranger(x = tr_x, y = as.factor(tr_y_full),
                 mtry = 50, num.trees = 2500,
                 importance = "impurity", probability = TRUE)

vip(rf_mod, n=20)

# Predict probabilities of class 1 (YES) for the validation set
preds_rf <- predict(rf_mod, data = va_x, type = "response")$predictions[, 2]

# Convert probabilities to class labels (YES or NO)
class_preds <- ifelse(preds_rf > 0.464, "YES", "NO")
valid_classifications_rf <- as.factor(class_preds)

```

```{r RF_graph}
library(randomForest)

tr_x <- X_train[-va_inds_rf,]
va_x <- X_train[va_inds_rf,] 

library(randomForest)

# Define a sequence of complexity levels (e.g., number of trees)
num_trees <- seq(1000, 3500, by = 500)

# Initialize vectors to store training and validation accuracies
train_accuracy <- numeric(length = length(num_trees))
valid_accuracy <- numeric(length = length(num_trees))

# Loop through each complexity level
for (i in seq_along(num_trees)) {
  # Train the random forest model
  rf_model <- ranger(x = tr_x, y = as.factor(tr_y_rf),
                     mtry = 50, num.trees = num_trees[i],
                     importance = "impurity", probability = TRUE)
  
  # Predict class labels for the training set
  preds_train <- predict(rf_model, data = tr_x, type = "response")$predictions[, 2]
  
  # Calculate training accuracy and store
  train_accuracy[i] <- mean(ifelse(preds_train > 0.464, "YES", "NO") == tr_y_rf)
  
  # Predict class labels for the validation set
  preds_valid <- predict(rf_model, data = va_x, type = "response")$predictions[, 2]
  
  # Calculate validation accuracy and store
  valid_accuracy[i] <- mean(ifelse(preds_valid > 0.464, "YES", "NO") == va_y_rf)
}

# Plot model complexity vs. performance
plot(num_trees, train_accuracy, type = "l", 
     xlab = "Number of Trees", ylab = "Accuracy",
     col = "blue", ylim = range(c(train_accuracy, valid_accuracy)),
     main = "Model Complexity vs. Performance (Random Forest)")
lines(num_trees, valid_accuracy, type = "l", col = "red")
legend("topright", legend = c("Training Accuracy", "Validation Accuracy"),
       col = c("blue", "red"), lty = 1, cex = 0.8)

# Plot training accuracy
plot(num_trees, train_accuracy, type = "l", 
     xlab = "Number of Trees", ylab = "Training Accuracy",
     col = "blue", ylim = c(0.95, 1), 
     main = "Model Complexity vs. Training Accuracy (Random Forest)")

# Plot validation accuracy
plot(num_trees, valid_accuracy, type = "l", 
     xlab = "Number of Trees", ylab = "Validation Accuracy",
     col = "red", ylim = c(0.70, 0.75),  
     main = "Model Complexity vs. Validation Accuracy (Random Forest)")

```


```{r 6th model: XGBoost, echo= False, warning = True}
train_clean_new <- train_clean[sapply(train_clean, is.numeric)]
train_clean_xg <- train_clean_new[1:82067, ]
valid_clean_xg <- train_clean_new[82068:nrow(train_clean_new), ] 
##train-test split (80%-20% split)
va_inds_xg <- sample(nrow(train_clean_xg), .2*nrow(train_clean_xg))

tr_xg <- train_clean_xg[-va_inds_xg,]
va_xg <- train_clean_xg[va_inds_xg,]
train_clean_check <- train_clean[1:82067, ]
valid_clean_check <- train_clean[82068:nrow(train_clean), ] 
tr_y_xg <- train_clean_check[-va_inds_xg,]$perfect_rating_score
va_y_xg <- train_clean_check[va_inds_xg,]$perfect_rating_score

tr_full_y_xg <- train_clean_check$perfect_rating_score
va_full_y_xg <- valid_clean_check$perfect_rating_score

# Convert labels to binary numeric values
tr_y_xg <- ifelse(tr_y_xg == "YES", 1, 0)
va_y_xg <- ifelse(va_y_xg == "YES", 1, 0)


# Train an XGBoost model
xgb_model <- xgboost(data = as.matrix(tr_xg), label = tr_y_xg, nrounds = 1000, objective = "binary:logistic")

#Training performance
predictions <- predict(xgb_model, as.matrix(tr_xg))

classifications <- ifelse(predictions > 0.5, 1, 0)
valid_classifications_xg <- as.factor(classifications)
tr_check_xg = factor(tr_y_xg)

# Check levels of tr_check_xg
levels(tr_check_xg)

# Check unique values in valid_classifications_xg
unique(valid_classifications_xg)

CM_train_xg<- confusionMatrix(data = valid_classifications_xg,
                      reference = tr_check_xg,
                      positive = "1"
                      )
TPR_train_xg <- as.numeric(CM_train_xg$byClass["Sensitivity"])
FPR_train_xg <- 1 - as.numeric(CM_train_xg$byClass["Specificity"])

#Generalization performance
predictions <- predict(xgb_model, as.matrix(va_xg))

classifications <- ifelse(predictions > 0.5, 1 , 0)

valid_classifications_xg <- as.factor(classifications)
va_check_xg = factor(va_y_xg)


CM_xg<- confusionMatrix(data = valid_classifications_xg, #predictions
                      reference = va_check_xg,
                      positive = "1"#actuals
                      )
TPR_xg <- as.numeric(CM_xg$byClass["Sensitivity"])
FPR_xg <- 1 - as.numeric(CM_xg$byClass["Specificity"])

##Holdout performance
predictions <- predict(xgb_model, as.matrix(valid_clean_xg))

classifications <- ifelse(predictions > 0.5, 1, 0)

valid_classifications_full_xg <- as.factor(classifications)
va_full_check_xg = factor(va_full_y_xg)


va_full_y_xg <- ifelse(va_full_y_xg == "YES", 1, 0)
va_full_y_xg <- as.factor(va_full_y_xg)

levels(valid_classifications_full_xg)
levels(va_full_y_xg)

# Calculate confusion matrix
CM_full_xg <- confusionMatrix(data = valid_classifications_full_xg,
                               reference = va_full_y_xg,
                               positive = "1"
                              )

TPR_full_xg <- as.numeric(CM_full_xg$byClass["Sensitivity"])
FPR_full_xg <- 1 - as.numeric(CM_full_xg$byClass["Specificity"])
```

```{r logistic model training performance results + generalization performance results}
cat("Training performance of XGBoost model: TPR:", TPR_train_xg, "\n")
cat("Training performance of XGBoost model: FPR:", FPR_train_xg, "\n")
cat("Generalization performance of XGBoost model on validation data: TPR:", TPR_xg, "\n")
cat("Generalization performance of XGBoost model on validation data: FPR:", FPR_xg, "\n")
cat("Generalization performance of XGBoost model on holdout data: TPR:", TPR_full_xg, "\n")
cat("Generalization performance of XGBoost model on holdout data: FPR:", FPR_full_xg, "\n")

```

```{r XGBoost_graph, echo=FALSE}
# Define a sequence of rounds (e.g., number of rounds)
nrounds <- seq(500, 1500, by = 500)

# Initialize vectors to store training and validation accuracies
train_accuracy <- numeric(length = length(nrounds))
valid_accuracy <- numeric(length = length(nrounds))

# Convert labels to binary numeric values
#tr_y_xg <- ifelse(tr_y_xg == "YES", 1, 0)
#va_y_xg <- ifelse(va_y_xg == "YES", 1, 0)

levels(valid_classifications_train) <- c("0", "1")
# Loop through each value of nrounds
for (i in seq_along(nrounds)) {
  # Train an XGBoost model with the current number of rounds
  xgb_model <- xgboost(data = as.matrix(tr_xg), label = tr_y_xg, nrounds = nrounds[i], objective = "binary:logistic")
  
  # Training performance
  predictions_train <- predict(xgb_model, as.matrix(tr_xg))
  classifications_train <- ifelse(predictions_train > 0.5, 1, 0)
  valid_classifications_train <- as.factor(classifications_train)
  tr_check_xg = factor(tr_y_xg)
  
  # Calculate training accuracy
  CM_train_xg <- confusionMatrix(data = valid_classifications_train,
                                reference = tr_check_xg,
                                positive = "1")
  train_accuracy[i] <- CM_train_xg$overall['Accuracy']
  
  # Generalization performance
  predictions_valid <- predict(xgb_model, as.matrix(va_xg))
  classifications_valid <- ifelse(predictions_valid > 0.5, 1, 0)
  valid_classifications_valid <- as.factor(classifications_valid)
  va_check_xg <- factor(va_y_xg)
  
  # Calculate validation accuracy
  CM_valid_xg <- confusionMatrix(data = valid_classifications_valid,
                                  reference = va_check_xg,
                                  positive = "1")
  valid_accuracy[i] <- CM_valid_xg$overall['Accuracy']
}

# Plot training and validation accuracy versus number of rounds
plot(nrounds, train_accuracy, type = "l", col = "blue", xlab = "Number of Rounds", ylab = "Accuracy", main = "Training and Validation Accuracy vs. Number of Rounds")
lines(nrounds, valid_accuracy, type = "l", col = "red")
legend("bottomright", legend = c("Training Accuracy", "Validation Accuracy"), col = c("blue", "red"), lty = 1, cex = 0.8)

```

```{r cutoff value}
# Increase granularity of cutoff value range
# Increase granularity of cutoff value range
cutoff_values <- seq(0.1, 0.9, by = 0.01)
va_full_y_rf <- valid_clean_rf$perfect_rating_score

# Initialize vectors to store metrics
accuracy_list <- numeric(length(cutoff_values))
precision_list <- numeric(length(cutoff_values))
TPR_list <- numeric(length(cutoff_values))
FPR_list <- numeric(length(cutoff_values))
length(va_y)
# Loop over cutoff values
for (i in seq_along(cutoff_values)) {
  # Apply cutoff value to predictions
  class_preds_cutoff <- ifelse(preds_rf > cutoff_values[i], "YES", "NO")
  
  # Calculate confusion matrix
  confusion_mat <- table(class_preds_cutoff, va_full_y_rf)
  
  # Ensure both classes are present in the confusion matrix
  if ("YES" %in% rownames(confusion_mat) && "NO" %in% colnames(confusion_mat)) {
    # Calculate true positives, true negatives, false positives, false negatives
    TP <- confusion_mat["YES", "YES"]
    TN <- confusion_mat["NO", "NO"]
    FP <- confusion_mat["YES", "NO"]
    FN <- confusion_mat["NO", "YES"]
    
    # Calculate accuracy
    accuracy_list[i] <- (TP + TN) / sum(confusion_mat)
    
    # Calculate precision
    precision_list[i] <- TP / (TP + FP)
    
    # Calculate recall (sensitivity)
    TPR_list[i] <- TP / (TP + FN)
    
    # Calculate F1 score
    FPR_list[i] <- FP / (FP + TN)
  } else {
    # Handle the case where one or both classes are missing
    # Set metrics to NA
    accuracy_list[i] <- NA
    precision_list[i] <- NA
    TPR_list[i] <- NA
    FPR_list[i] <- NA
  }
}

# Print or visualize the performance metrics for each cutoff value
metrics_df <- data.frame(Cutoff = cutoff_values, Accuracy = accuracy_list, Precision = precision_list, TPR = TPR_list, FPR = FPR_list)
print(metrics_df)

# Optionally, plot the performance metrics against cutoff values
plot(cutoff_values, accuracy_list, type = "l", col = "blue", ylim = c(0, 1), xlab = "Cutoff Value", ylab = "Performance Metric", main = "Performance Metrics vs. Cutoff Value")
lines(cutoff_values, precision_list, type = "l", col = "red")
lines(cutoff_values, TPR_list, type = "l", col = "green")
lines(cutoff_values, FPR_list, type = "l", col = "orange")
legend("topright", legend = c("Accuracy", "Precision", "TPR", "FPR"), col = c("blue", "red", "green", "orange"), lty = 1)

# Determine the optimal cutoff value based on desired specificity
desired_specificity <- 0.095  # Desired false positive rate (7%)
cutoff_index <- which.min(abs(FPR_list - desired_specificity))
cutoff_value <- cutoff_values[cutoff_index]
cutoff_value
```


```{r generate_csv using random forest predictions}
#this code creates sample outputs in the correct format
write.table(valid_classifications_rf, "perfect_rating_score_group22.csv", row.names=FALSE)
```

